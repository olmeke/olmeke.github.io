---
layout: post
title:  "Week 6 - \"VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative Framework\""
date:   2025-02-07 01:50:15 -0700
categories: paper review
---

Last week, we explored a human-in-the-loop approach to penetration testing, where large language models (LLMs) provided assistance but still relied on human expertise to drive the process. This week, I came across a fresh and exciting research paper that pushes pentesting automation even further —

**VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative Framework**

by *He Kong, Die Hu, Jingguo Ge, Liangxiong Li, Tong Li, and Bingzhen Wu*.

#### TL;DR

Existing automated pentesting solutions come with trade-offs: some require human oversight, while others suffer from inefficient data processing. VulnBot proposes a multi-agent system where specialized roles collaborate across different pentesting phases, significantly improving automation and efficiency.

#### The Problem: Automating Pentesting Without Losing Context

Current LLM-powered pentesting solutions each have their own limitations:

* PentestGPT requires a human-in-the-loop to guide the process and interpret outputs.
* AutoAttacker is highly specialized and lacks generalization for broader real-world testing.

The challenge is clear: how can we design a system that is both fully autonomous and flexible enough to handle real-world penetration testing scenarios?

To answer this, the authors conducted a small study and identified four key bottlenecks:

1. Limited context window – LLMs forget important details over time.
2. Poor translation of natural language tasks into commands – Often leading to incorrect commands or errors.
3. Weak error handling – No robust mechanism for dealing with failed commands.
4. Lack of long-horizon reasoning – Pentesting is a multi-step process, requiring strategic execution across different phases.

#### The Solution: A Multi-Agent System with Specialized Roles

VulnBot tackles these problems by structuring pentesting into three core phases:

✅ Reconnaissance – Mapping the attack surface. \
✅ Scanning – Identifying vulnerabilities and misconfigurations. \
✅ Exploitation – Using the gathered information to attack the target. \

Each phase is powered by five core modules, designed to improve efficiency:

* Memory Retriever – Helps overcome context loss by storing relevant data.
* Planner – Generates a Penetration Testing Task Graph (PTG), ensuring logical execution order.
* Generator – Translates natural language instructions into tool-specific commands.
* Executor – Runs commands and interacts with the target system.
* Summarizer – Condenses information to ensure smooth inter-agent communication.

By breaking the problem into well-defined steps and assigning specialized roles, VulnBot ensures each phase builds upon the previous one, creating a more structured and efficient workflow.

**Key Design Innovations**

**1) Specialized Roles for Efficient Execution** \
By dividing tasks across reconnaissance, scanning, and exploitation, VulnBot reduces complexity and improves focus at each stage. This prevents LLMs from getting overwhelmed with too much context at once.

**2) Dedicated Planning with a Penetration Testing Task Graph (PTG)** \
The PTG is an acyclic directed graph, dynamically updated after every step to keep execution structured. The framework ensures that updates after each step are valid. The PTG allows VulnBot to:

* Ensure tasks are completed in a logical order.
* Adapt dynamically based on execution results.

**3) Summarizer Module for Efficient Communication** \
Pentesting requires passing information across phases and between modules. Instead of dumping raw logs or excessive details, VulnBot uses a Summarizer module to create concise, human-readable updates.
This is critical for keeping LLMs focused and avoiding unnecessary computation.

**4) Flexible Execution Modes** \
VulnBot supports three execution modes:

* Automatic – Fully autonomous execution. The Generator translates natural language instructions into tool commands, and the Executor runs them automatically.
* Manual – The user executes commands but receives guidance.
* Semi-Automatic – The planner decides when human intervention is needed.

This adaptability makes it useful in both research and practical security settings. 

#### Results: Does It Work?

**Benchmark Performance** \
VulnBot was evaluated on two major datasets:

* AUTOPENBENCH – A diverse set of pentesting tasks.
* AI-Pentest-Benchmark – Real-world vulnerability testing.

VulnBot outperformed all baseline models, including PentestGPT and traditional LLM-driven approaches. Particularly notable:

* On AUTOPENBENCH, it achieved a 30.3% completion rate, significantly higher than GPT-4o’s 21.2%.
* On AI-Pentest-Benchmark, VulnBot showed strong results, with DeepSeek-V3 performing particularly well in some cases.

**Ablation Study Insights** \
One surprising finding: removing the Summarizer caused the largest drop in performance, even more than removing the PTG. This suggests efficient inter-agent communication is a critical factor in pentesting success.

#### My Thoughts

One major gap in the paper is the lack of detail on the agents themselves—their specific capabilities, the tools they use, and how they interact beyond their defined roles. While the system is described at a high level, a deeper breakdown of each agent’s functionality would provide a clearer picture of the system.

Additionally, the evaluation appears to focus solely on web-based vulnerabilities, with no mention of binary exploitation. Given the importance of memory corruption and low-level vulnerabilities in real-world pentesting, including such tests would be crucial for understanding VulnBot’s full capabilities.

That said, the architecture is well-designed and highly adaptable—the same modular components power all three phases of pentesting. This suggests that a similar structure could be effective in other security-related or non-security related tasks.
